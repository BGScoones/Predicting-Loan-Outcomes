{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this project, we'll be going through the full process of data cleaning, data transformation, and finally model prediction, in order to predict whether a particular loan request is likely to be paid off. We'll be doing this with financial lending data from [Lending Club](https://www.lendingclub.com/info/download-data.action), an online marketplace for personal loans that matches borrowers with investors. Naturally, we'll be playing the part of an interested investor who's looking to make a profit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>member_id</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>funded_amnt</th>\n",
       "      <th>funded_amnt_inv</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>grade</th>\n",
       "      <th>sub_grade</th>\n",
       "      <th>...</th>\n",
       "      <th>last_pymnt_amnt</th>\n",
       "      <th>last_credit_pull_d</th>\n",
       "      <th>collections_12_mths_ex_med</th>\n",
       "      <th>policy_code</th>\n",
       "      <th>application_type</th>\n",
       "      <th>acc_now_delinq</th>\n",
       "      <th>chargeoff_within_12_mths</th>\n",
       "      <th>delinq_amnt</th>\n",
       "      <th>pub_rec_bankruptcies</th>\n",
       "      <th>tax_liens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1077501</td>\n",
       "      <td>1296599.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>4975.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>10.65%</td>\n",
       "      <td>162.87</td>\n",
       "      <td>B</td>\n",
       "      <td>B2</td>\n",
       "      <td>...</td>\n",
       "      <td>171.62</td>\n",
       "      <td>Jun-2016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1077430</td>\n",
       "      <td>1314167.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>15.27%</td>\n",
       "      <td>59.83</td>\n",
       "      <td>C</td>\n",
       "      <td>C4</td>\n",
       "      <td>...</td>\n",
       "      <td>119.66</td>\n",
       "      <td>Sep-2013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1077175</td>\n",
       "      <td>1313524.0</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>15.96%</td>\n",
       "      <td>84.33</td>\n",
       "      <td>C</td>\n",
       "      <td>C5</td>\n",
       "      <td>...</td>\n",
       "      <td>649.91</td>\n",
       "      <td>Jun-2016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1076863</td>\n",
       "      <td>1277178.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>13.49%</td>\n",
       "      <td>339.31</td>\n",
       "      <td>C</td>\n",
       "      <td>C1</td>\n",
       "      <td>...</td>\n",
       "      <td>357.48</td>\n",
       "      <td>Apr-2016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1075358</td>\n",
       "      <td>1311748.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>12.69%</td>\n",
       "      <td>67.79</td>\n",
       "      <td>B</td>\n",
       "      <td>B5</td>\n",
       "      <td>...</td>\n",
       "      <td>67.79</td>\n",
       "      <td>Jun-2016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  member_id  loan_amnt  funded_amnt  funded_amnt_inv        term  \\\n",
       "0  1077501  1296599.0     5000.0       5000.0           4975.0   36 months   \n",
       "1  1077430  1314167.0     2500.0       2500.0           2500.0   60 months   \n",
       "2  1077175  1313524.0     2400.0       2400.0           2400.0   36 months   \n",
       "3  1076863  1277178.0    10000.0      10000.0          10000.0   36 months   \n",
       "4  1075358  1311748.0     3000.0       3000.0           3000.0   60 months   \n",
       "\n",
       "  int_rate  installment grade sub_grade    ...    last_pymnt_amnt  \\\n",
       "0   10.65%       162.87     B        B2    ...             171.62   \n",
       "1   15.27%        59.83     C        C4    ...             119.66   \n",
       "2   15.96%        84.33     C        C5    ...             649.91   \n",
       "3   13.49%       339.31     C        C1    ...             357.48   \n",
       "4   12.69%        67.79     B        B5    ...              67.79   \n",
       "\n",
       "  last_credit_pull_d collections_12_mths_ex_med  policy_code application_type  \\\n",
       "0           Jun-2016                        0.0          1.0       INDIVIDUAL   \n",
       "1           Sep-2013                        0.0          1.0       INDIVIDUAL   \n",
       "2           Jun-2016                        0.0          1.0       INDIVIDUAL   \n",
       "3           Apr-2016                        0.0          1.0       INDIVIDUAL   \n",
       "4           Jun-2016                        0.0          1.0       INDIVIDUAL   \n",
       "\n",
       "  acc_now_delinq chargeoff_within_12_mths delinq_amnt pub_rec_bankruptcies  \\\n",
       "0            0.0                      0.0         0.0                  0.0   \n",
       "1            0.0                      0.0         0.0                  0.0   \n",
       "2            0.0                      0.0         0.0                  0.0   \n",
       "3            0.0                      0.0         0.0                  0.0   \n",
       "4            0.0                      0.0         0.0                  0.0   \n",
       "\n",
       "  tax_liens  \n",
       "0       0.0  \n",
       "1       0.0  \n",
       "2       0.0  \n",
       "3       0.0  \n",
       "4       0.0  \n",
       "\n",
       "[5 rows x 52 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"loans_2007.csv\", low_memory = False)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Cleaning\n",
    "\n",
    "The dataset has already been cleaned up to remove some problematic or unnecessary columns, and remove columns with more than 50% of the values missing. However, we still need to look more closely through the columns for a few things:\n",
    "* Whether the column leaks information about the future\n",
    "* Whether the column is irrelevant to the borrower's ability to pay back the loan\n",
    "* Whether a column needs to be reformatted\n",
    "* Whether a column needs to be processed in order for useful information to be extracted from it\n",
    "* Whether the column contains redundant information that is present elsewhere in the dataset\n",
    "\n",
    "Of these, the first is the most important. If a column leaks information about the future, then if use it as a feature in our model for prediction we'll end up with an overfit model that won't be any good for predicting information on future loans (since we won't have that information yet).\n",
    "\n",
    "We also need to select a column as a target column which we'll be trying to predict with our model.\n",
    "\n",
    "To find more detailed information on the columns in our dataset, we can look at the data dictionary provided by Lending Club, found [here](https://docs.google.com/spreadsheets/d/191B2yJ4H1ZPXq0_ByhUgWMFZOYem5jFz0Y3by_7YBY4/edit#gid=2081333097)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'member_id',\n",
       " 'loan_amnt',\n",
       " 'funded_amnt',\n",
       " 'funded_amnt_inv',\n",
       " 'term',\n",
       " 'int_rate',\n",
       " 'installment',\n",
       " 'grade',\n",
       " 'sub_grade',\n",
       " 'emp_title',\n",
       " 'emp_length',\n",
       " 'home_ownership',\n",
       " 'annual_inc',\n",
       " 'verification_status',\n",
       " 'issue_d',\n",
       " 'loan_status',\n",
       " 'pymnt_plan',\n",
       " 'purpose',\n",
       " 'title',\n",
       " 'zip_code',\n",
       " 'addr_state',\n",
       " 'dti',\n",
       " 'delinq_2yrs',\n",
       " 'earliest_cr_line',\n",
       " 'inq_last_6mths',\n",
       " 'open_acc',\n",
       " 'pub_rec',\n",
       " 'revol_bal',\n",
       " 'revol_util',\n",
       " 'total_acc',\n",
       " 'initial_list_status',\n",
       " 'out_prncp',\n",
       " 'out_prncp_inv',\n",
       " 'total_pymnt',\n",
       " 'total_pymnt_inv',\n",
       " 'total_rec_prncp',\n",
       " 'total_rec_int',\n",
       " 'total_rec_late_fee',\n",
       " 'recoveries',\n",
       " 'collection_recovery_fee',\n",
       " 'last_pymnt_d',\n",
       " 'last_pymnt_amnt',\n",
       " 'last_credit_pull_d',\n",
       " 'collections_12_mths_ex_med',\n",
       " 'policy_code',\n",
       " 'application_type',\n",
       " 'acc_now_delinq',\n",
       " 'chargeoff_within_12_mths',\n",
       " 'delinq_amnt',\n",
       " 'pub_rec_bankruptcies',\n",
       " 'tax_liens']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by cutting some columns. The following columns we'll cut because of data leakage: \"funded_amnt\", \"funded_amnt_inv\", \"issue_d\", \"out_prncp\", \"out_prncp_inv\", \"total_pymnt\", \"total_pymnt_inv\", \"total_rec_prncp\", \"total_rec_int\", \"total_rec_late_fee\", \"recoveries\", \"collection_recovery_fee\", \"last_pymnt_d\", \"last_pymnt_amnt\", \"last_credit_pull_d\".\n",
    "\n",
    "We'll cut the following because they're irrelevant, redundant, or hard to transform: \"id\", \"member_id\", \"grade\" (redundant, info in int_rate), \"sub_grade\" (ditto), \"zip_code\" (redundant, info in addr_state), \"emp_title\" (hard to transform).\n",
    "\n",
    "While loan_status leaks information about the future, we need to make sure we keep it since it's going to serve as our target column. Our goal, after all, is to see which loan requests are worth taking because we expect the borrower to pay off the loan successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42538, 31)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop([\"id\", \"member_id\", \"funded_amnt\", \"funded_amnt_inv\", \"grade\", \"sub_grade\", \"emp_title\", \"issue_d\", \"zip_code\", \"out_prncp\", \"out_prncp_inv\", \"total_pymnt\", \"total_pymnt_inv\", \"total_rec_prncp\", \"total_rec_int\", \"total_rec_late_fee\", \"recoveries\", \"collection_recovery_fee\", \"last_pymnt_d\", \"last_pymnt_amnt\", \"last_credit_pull_d\"], axis = 1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few columns we may need to transform, such as purpose, title, and addr_state, but we'll leave those for now.\n",
    "\n",
    "### Cleaning the Target Column\n",
    "\n",
    "Let's now look more closely at our target column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fully Paid                                             33136\n",
       "Charged Off                                             5634\n",
       "Does not meet the credit policy. Status:Fully Paid      1988\n",
       "Current                                                  961\n",
       "Does not meet the credit policy. Status:Charged Off      761\n",
       "Late (31-120 days)                                        24\n",
       "In Grace Period                                           20\n",
       "Late (16-30 days)                                          8\n",
       "Default                                                    3\n",
       "Name: loan_status, dtype: int64"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"loan_status\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're trying to predict which loans will be successfully paid off, we're only interested in those loans which are either Full Paid or Charged Off. Those loans with are classified as Current, In Grace Period, Late (16 - 30 days), Late (31 -120 days), and Default are still undetermined.\n",
    "\n",
    "We're also not interested in those loans which were fully paid or charged off but which no longer meet Lending Club's policy. Such loans will no longer get approved, and so if we include the data in those rows we'll be fitting our model to data which will be irrelevant in the future, because it relates to loans that will no longer meet the policy and we will therefore never need to evaluate.\n",
    "\n",
    "This therefore turns our problem into one of binary classification. We're trying to predict whether a given loan will result in one of two values, either fully paid or charged off. We'll remove the rows for those loans whose statuses fall into neither category, and then we'll replace Fully Paid values with 1, and Charged Off with 0.\n",
    "\n",
    "It should be noted that doing the above results in a class imbalance between the two outcomes. That is, the dataset we're training our model on contains far more loans which were fully paid than those that were charged off. This can result in the model having a bias towards predicting the class with more observations, in this case, full paid. This would be a disaster for us.\n",
    "\n",
    "While we want to make as much money as possible by evaluating each prospective loan correctly, if the model were skewed towards predicting charged off rather than fully paid, this would be less of a concern. We'd miss out on funding some loans that would actually be good due to the bias of our model, but that would just mean less profit. However, given that our model would be biased towards predicting fully paid, we'd instead be funding loans which were actually bad, which would not just mean less profit, but potentially a loss.\n",
    "\n",
    "As such, we'll need to remedy this issue, but we'll do so later. For now, we'll go ahead and alter our dataframe to contain only rows with loan statuses of fully paid or charged off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[(data['loan_status'] == \"Fully Paid\") | (data['loan_status'] == \"Charged Off\")]\n",
    "\n",
    "status_replace = {\n",
    "    \"loan_status\" : {\n",
    "        \"Fully Paid\": 1,\n",
    "        \"Charged Off\": 0,\n",
    "    }\n",
    "}\n",
    "\n",
    "data = data.replace(status_replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Useless Columns\n",
    "\n",
    "Some columns may contain only one unique value. These columns will be useless for modelling as they contain no information to differentiate the loans from one another. To remove them, we first need to drop any null values from the columns, then find whether the remaining values are all alike. If they are, we'll add the column name to a list, and finally remove from the dataframe all columns in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pymnt_plan', 'initial_list_status', 'collections_12_mths_ex_med', 'policy_code', 'application_type', 'acc_now_delinq', 'chargeoff_within_12_mths', 'delinq_amnt', 'tax_liens']\n"
     ]
    }
   ],
   "source": [
    "drop_columns = []\n",
    "\n",
    "for col in list(data):\n",
    "    non_null_unique = data[col].dropna().unique()\n",
    "    if len(non_null_unique) == 1:\n",
    "        drop_columns.append(col)\n",
    "\n",
    "data = data.drop(drop_columns, axis = 1)\n",
    "\n",
    "print(drop_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all of this, our dataframe is looking much smaller and easier to manage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   loan_amnt        term int_rate  installment emp_length home_ownership  \\\n",
      "0     5000.0   36 months   10.65%       162.87  10+ years           RENT   \n",
      "1     2500.0   60 months   15.27%        59.83   < 1 year           RENT   \n",
      "2     2400.0   36 months   15.96%        84.33  10+ years           RENT   \n",
      "3    10000.0   36 months   13.49%       339.31  10+ years           RENT   \n",
      "5     5000.0   36 months    7.90%       156.46    3 years           RENT   \n",
      "\n",
      "   annual_inc verification_status  loan_status         purpose  \\\n",
      "0     24000.0            Verified            1     credit_card   \n",
      "1     30000.0     Source Verified            0             car   \n",
      "2     12252.0        Not Verified            1  small_business   \n",
      "3     49200.0     Source Verified            1           other   \n",
      "5     36000.0     Source Verified            1         wedding   \n",
      "\n",
      "          ...             dti delinq_2yrs  earliest_cr_line  inq_last_6mths  \\\n",
      "0         ...           27.65         0.0          Jan-1985             1.0   \n",
      "1         ...            1.00         0.0          Apr-1999             5.0   \n",
      "2         ...            8.72         0.0          Nov-2001             2.0   \n",
      "3         ...           20.00         0.0          Feb-1996             1.0   \n",
      "5         ...           11.20         0.0          Nov-2004             3.0   \n",
      "\n",
      "  open_acc  pub_rec  revol_bal  revol_util  total_acc pub_rec_bankruptcies  \n",
      "0      3.0      0.0    13648.0       83.7%        9.0                  0.0  \n",
      "1      3.0      0.0     1687.0        9.4%        4.0                  0.0  \n",
      "2      2.0      0.0     2956.0       98.5%       10.0                  0.0  \n",
      "3     10.0      0.0     5598.0         21%       37.0                  0.0  \n",
      "5      9.0      0.0     7963.0       28.3%       12.0                  0.0  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "(38770, 22)\n"
     ]
    }
   ],
   "source": [
    "print(data.head())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transforming the Data\n",
    "\n",
    "Now that we've selected the columns we'll be using in our model, we need to make sure the data is in suitable form. We'll remove missing values and convert non-numerical data to numerical data, before giving our data one final look over to see if there are any other columns we can remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loan_amnt                  0\n",
       "term                       0\n",
       "int_rate                   0\n",
       "installment                0\n",
       "emp_length              1036\n",
       "home_ownership             0\n",
       "annual_inc                 0\n",
       "verification_status        0\n",
       "loan_status                0\n",
       "purpose                    0\n",
       "title                     11\n",
       "addr_state                 0\n",
       "dti                        0\n",
       "delinq_2yrs                0\n",
       "earliest_cr_line           0\n",
       "inq_last_6mths             0\n",
       "open_acc                   0\n",
       "pub_rec                    0\n",
       "revol_bal                  0\n",
       "revol_util                50\n",
       "total_acc                  0\n",
       "pub_rec_bankruptcies     697\n",
       "dtype: int64"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_counts = data.isnull().sum()\n",
    "null_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four columns with missing values: emp_length, title, revol_util, and pub_rec_bankruptcies. The missing rows in title and revol_util represent such a small portion of the dataset that we can safely remove those rows without significantly impacting the data.\n",
    "\n",
    "The emp_length and pub_rec_bankruptcies columns are a little more difficult, as rows with missing values in either of these columns represents a much larger proportion of the dataset. We have three options for dealing with these missing values. We could remove the rows with missing values in either of these two columns, we could remove the columns themselves, or we could fill in the missing values with an average of the values in each column. Let's take a closer look at these two columns to get a better idea of the information they contain before we make our decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10+ years    8547\n",
       "< 1 year     4527\n",
       "2 years      4308\n",
       "3 years      4026\n",
       "4 years      3362\n",
       "5 years      3209\n",
       "1 year       3183\n",
       "6 years      2181\n",
       "7 years      1718\n",
       "8 years      1444\n",
       "9 years      1229\n",
       "Name: emp_length, dtype: int64"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_length_counts = data[\"emp_length\"].value_counts()\n",
    "emp_length_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    36422\n",
       "1.0     1646\n",
       "2.0        5\n",
       "Name: pub_rec_bankruptcies, dtype: int64"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pub_rec_bank_counts = data[\"pub_rec_bankruptcies\"].value_counts()\n",
    "pub_rec_bank_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first option above (removing rows with missing values) is unappealing because we'll lose a lot of other information in the dataset that those rows contain.\n",
    "\n",
    "The second option (removing the columns) looks like a better option for the pub_rec_bank_counts column. It would be hard to replace the missing values with any kind of average, as the average in all cases would be 0. That might be a realistic prediction for most of those values, but we have no way of knowing whether that is the case. Furthermore, since so many of the values in the column are 0, we're not losing that much information by removing the column in its entirety. This may turn out to be a mistake if pub_rec_bankruptcies is strongly correlated with loan status, but we'll proceed by removing the column for now.\n",
    "\n",
    "As for emp_length, the second option also looks reasonable. Assigning a mean value would be impossible without dealing with the 10+ years column (as we have no idea how many years precisely each individual in this category has been employed for). Assigning modal and median values would be possible, but in neither case would be representative of the range of values in emp_length. Prima facie, we also wouldn't expect emp_length to be that strongly correlated with loan_status. So we'll cut this column as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop([\"pub_rec_bankruptcies\", \"emp_length\"], axis = 1)\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to convert text columns to numerical data types. We'll look more closely at the non-numeric columns in our dataset using the select_dtypes method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64    10\n",
      "object      9\n",
      "int64       1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_df = data.select_dtypes(include = [\"object\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>verification_status</th>\n",
       "      <th>purpose</th>\n",
       "      <th>title</th>\n",
       "      <th>addr_state</th>\n",
       "      <th>earliest_cr_line</th>\n",
       "      <th>revol_util</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36 months</td>\n",
       "      <td>10.65%</td>\n",
       "      <td>RENT</td>\n",
       "      <td>Verified</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>Computer</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Jan-1985</td>\n",
       "      <td>83.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60 months</td>\n",
       "      <td>15.27%</td>\n",
       "      <td>RENT</td>\n",
       "      <td>Source Verified</td>\n",
       "      <td>car</td>\n",
       "      <td>bike</td>\n",
       "      <td>GA</td>\n",
       "      <td>Apr-1999</td>\n",
       "      <td>9.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36 months</td>\n",
       "      <td>15.96%</td>\n",
       "      <td>RENT</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>small_business</td>\n",
       "      <td>real estate business</td>\n",
       "      <td>IL</td>\n",
       "      <td>Nov-2001</td>\n",
       "      <td>98.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36 months</td>\n",
       "      <td>13.49%</td>\n",
       "      <td>RENT</td>\n",
       "      <td>Source Verified</td>\n",
       "      <td>other</td>\n",
       "      <td>personel</td>\n",
       "      <td>CA</td>\n",
       "      <td>Feb-1996</td>\n",
       "      <td>21%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>36 months</td>\n",
       "      <td>7.90%</td>\n",
       "      <td>RENT</td>\n",
       "      <td>Source Verified</td>\n",
       "      <td>wedding</td>\n",
       "      <td>My wedding loan I promise to pay back</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Nov-2004</td>\n",
       "      <td>28.3%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         term int_rate home_ownership verification_status         purpose  \\\n",
       "0   36 months   10.65%           RENT            Verified     credit_card   \n",
       "1   60 months   15.27%           RENT     Source Verified             car   \n",
       "2   36 months   15.96%           RENT        Not Verified  small_business   \n",
       "3   36 months   13.49%           RENT     Source Verified           other   \n",
       "5   36 months    7.90%           RENT     Source Verified         wedding   \n",
       "\n",
       "                                   title addr_state earliest_cr_line  \\\n",
       "0                               Computer         AZ         Jan-1985   \n",
       "1                                   bike         GA         Apr-1999   \n",
       "2                   real estate business         IL         Nov-2001   \n",
       "3                               personel         CA         Feb-1996   \n",
       "5  My wedding loan I promise to pay back         AZ         Nov-2004   \n",
       "\n",
       "  revol_util  \n",
       "0      83.7%  \n",
       "1       9.4%  \n",
       "2      98.5%  \n",
       "3        21%  \n",
       "5      28.3%  "
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two easy things to do are: to convert the int_rate and revol_util columns, as both contain numeric values; to remove the earliest_cr_line column, as this contains a date value which would require some alteration to make useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"int_rate\"] = data[\"int_rate\"].str.rstrip(\"%\").astype(\"float\")\n",
    "data[\"revol_util\"] = data[\"revol_util\"].str.rstrip(\"%\").astype(\"float\")\n",
    "\n",
    "data = data.drop([\"earliest_cr_line\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now check to see if any of the columns are categorical, as most look as if they may be. We can check this by looking at the number of unique values in each column (and also by looking at the documentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'term':  36 months    29042\n",
       "  60 months     9667\n",
       " Name: term, dtype: int64, 'home_ownership': RENT        18514\n",
       " MORTGAGE    17112\n",
       " OWN          2984\n",
       " OTHER          96\n",
       " NONE            3\n",
       " Name: home_ownership, dtype: int64, 'verification_status': Not Verified       16698\n",
       " Verified           12289\n",
       " Source Verified     9722\n",
       " Name: verification_status, dtype: int64, 'purpose': debt_consolidation    18130\n",
       " credit_card            5039\n",
       " other                  3865\n",
       " home_improvement       2897\n",
       " major_purchase         2154\n",
       " small_business         1763\n",
       " car                    1510\n",
       " wedding                 929\n",
       " medical                 680\n",
       " moving                  576\n",
       " vacation                375\n",
       " house                   369\n",
       " educational             320\n",
       " renewable_energy        102\n",
       " Name: purpose, dtype: int64, 'title': Debt Consolidation                         2104\n",
       " Debt Consolidation Loan                    1632\n",
       " Personal Loan                               642\n",
       " Consolidation                               494\n",
       " debt consolidation                          485\n",
       " Credit Card Consolidation                   353\n",
       " Home Improvement                            346\n",
       " Debt consolidation                          324\n",
       " Small Business Loan                         310\n",
       " Credit Card Loan                            305\n",
       " Personal                                    302\n",
       " Consolidation Loan                          251\n",
       " Home Improvement Loan                       234\n",
       " personal loan                               227\n",
       " personal                                    211\n",
       " Loan                                        208\n",
       " Wedding Loan                                201\n",
       " Car Loan                                    195\n",
       " consolidation                               193\n",
       " Other Loan                                  181\n",
       " Credit Card Payoff                          150\n",
       " Wedding                                     149\n",
       " Credit Card Refinance                       143\n",
       " Major Purchase Loan                         139\n",
       " Consolidate                                 125\n",
       " Medical                                     118\n",
       " Credit Card                                 115\n",
       " home improvement                            107\n",
       " My Loan                                      92\n",
       " Credit Cards                                 91\n",
       "                                            ... \n",
       " Regain financial independence!                1\n",
       " Stop paying interest to Chase                 1\n",
       " Refinance High Interest Rates                 1\n",
       " credit resolutions                            1\n",
       " Family Dept Consolidation Loan                1\n",
       " Family Home Improvement                       1\n",
       " lower rate loan                               1\n",
       " Bill pay                                      1\n",
       " For a Bike                                    1\n",
       " Amazon Card Refi                              1\n",
       " SB Loan for start-up expenses                 1\n",
       " Dec2310 Loan                                  1\n",
       " Patio Improvement                             1\n",
       " Daryl's loan                                  1\n",
       " payoffdiscover                                1\n",
       " Consolidating My Debts = Much Easier          1\n",
       " drbajhf                                       1\n",
       " Como Help                                     1\n",
       " car needs help                                1\n",
       " Credit Card refinance from 20+%               1\n",
       " Balance transfer AT & T                       1\n",
       " Smart                                         1\n",
       " Baby Expenses                                 1\n",
       " paydebts                                      1\n",
       " Aprilia                                       1\n",
       " PAYOFF CREDIT CARDS, LOAN and CASH FLOW       1\n",
       " Pay off Medical Bills & Be Debt Free!         1\n",
       "  debt consolidation                           1\n",
       " 2nd story expansion                           1\n",
       " MJM                                           1\n",
       " Name: title, Length: 19333, dtype: int64, 'addr_state': CA    6960\n",
       " NY    3713\n",
       " FL    2791\n",
       " TX    2667\n",
       " NJ    1798\n",
       " IL    1483\n",
       " PA    1473\n",
       " VA    1376\n",
       " GA    1364\n",
       " MA    1301\n",
       " OH    1179\n",
       " MD    1026\n",
       " AZ     850\n",
       " WA     822\n",
       " CO     770\n",
       " NC     753\n",
       " CT     730\n",
       " MI     712\n",
       " MO     670\n",
       " MN     603\n",
       " NV     481\n",
       " SC     462\n",
       " WI     441\n",
       " AL     437\n",
       " OR     436\n",
       " LA     430\n",
       " KY     315\n",
       " OK     290\n",
       " KS     260\n",
       " UT     254\n",
       " AR     237\n",
       " DC     209\n",
       " RI     196\n",
       " NM     184\n",
       " WV     172\n",
       " NH     166\n",
       " HI     166\n",
       " DE     113\n",
       " MT      83\n",
       " WY      80\n",
       " AK      78\n",
       " SD      61\n",
       " VT      53\n",
       " MS      19\n",
       " TN      17\n",
       " IN       9\n",
       " ID       6\n",
       " NE       5\n",
       " IA       5\n",
       " ME       3\n",
       " Name: addr_state, dtype: int64}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = [\"term\", \"home_ownership\", \"verification_status\", \"purpose\", \"title\", \"addr_state\"]\n",
    "\n",
    "values = {}\n",
    "\n",
    "for col in cols:\n",
    "    values[col] = object_df[col].value_counts()\n",
    "    \n",
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the columns contain a reasonable number of discrete categories and so can be encoded as dummy variables to be used in our model. However, the addr_state column contains many discrete values, and so would require adding many new columns if we were to represent the information in the column with dummy variables. We'll cut the column for that reason.\n",
    "\n",
    "Additionally, purpose and title seem to contain similar information, and many of the different values in title are unique given that the borrower can give their loan application any title. Thus, we'll remove the title column and keep the purpose column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop([\"addr_state\", \"title\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loan_amnt',\n",
       " 'term',\n",
       " 'int_rate',\n",
       " 'installment',\n",
       " 'home_ownership',\n",
       " 'annual_inc',\n",
       " 'verification_status',\n",
       " 'loan_status',\n",
       " 'purpose',\n",
       " 'dti',\n",
       " 'delinq_2yrs',\n",
       " 'inq_last_6mths',\n",
       " 'open_acc',\n",
       " 'pub_rec',\n",
       " 'revol_bal',\n",
       " 'revol_util',\n",
       " 'total_acc',\n",
       " 'term_ 36 months',\n",
       " 'term_ 60 months',\n",
       " 'home_ownership_MORTGAGE',\n",
       " 'home_ownership_NONE',\n",
       " 'home_ownership_OTHER',\n",
       " 'home_ownership_OWN',\n",
       " 'home_ownership_RENT',\n",
       " 'verification_status_Not Verified',\n",
       " 'verification_status_Source Verified',\n",
       " 'verification_status_Verified',\n",
       " 'purpose_car',\n",
       " 'purpose_credit_card',\n",
       " 'purpose_debt_consolidation',\n",
       " 'purpose_educational',\n",
       " 'purpose_home_improvement',\n",
       " 'purpose_house',\n",
       " 'purpose_major_purchase',\n",
       " 'purpose_medical',\n",
       " 'purpose_moving',\n",
       " 'purpose_other',\n",
       " 'purpose_renewable_energy',\n",
       " 'purpose_small_business',\n",
       " 'purpose_vacation',\n",
       " 'purpose_wedding']"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_cols = [\"term\", \"home_ownership\", \"verification_status\", \"purpose\"]\n",
    "\n",
    "dummy_df = pd.get_dummies(data[cat_cols])\n",
    "data = pd.concat([data, dummy_df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop([\"term\", \"home_ownership\", \"verification_status\", \"purpose\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now done with data cleaning and transformation, which means we're ready to start making models to predict whether a loan will be paid off on time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Making Models\n",
    "\n",
    "Before we make our model, we need to decide on an error metric which we can use to estimate the accuracy of our model, and to compare the accuracy of models to one another.\n",
    "\n",
    "Above, we filtered the data in the loan_status column so that it only contained those loans which had either been paid off or which were never going to be paid off. In doing this, we turned our loan_status column into a binary column, thereby making our goal one of binary classification.\n",
    "\n",
    "In this instance, our goal is to make money. To do this, we'll need to identify accurately both loans that will be paid off (which we can take on) and loans that will not be paid off (which we can avoid). We therefore want our model to avoid making predictions that are false positives or false negatives. A false positive occurs when our model predicts that a loan was paid off, but it actually was not paid off. A false negative occurs when our model predicts that a loan was notpaid off, but it actually was paid off. Each false positive results in us losing money, because we take a loan which will not be paid off. Each false negative results in us missing out on money, because we decided not to take a loan that actually would have been paid off.\n",
    "\n",
    "As discussed above, false positives are far worse for us than false negatives, because at least with a false negative we don't lose money, we only forego profit. However, we want to maximize profit, and so we want to make sure we're taking advantage of every favorable opportunity, and avoiding every unfavorable opportunity as much as possible. We therefore want to minimize the ratio of false positives to true positives, and the ratio of false negatives to true negatives.\n",
    "\n",
    "N.B. Maximizing profit is actually a little more complex than this. In general, we want to avoid false positives and false negatives, but in reality we're making a model that will involve trade-offs. We may be able to make a model that has a higher number of false positives, but also identifies high-value true positives at a high rate. Such a model may make more money than one which has a lower number of false positives, but is bad at identifying high-value true positives. Additionally, we care more about high-value loans than low-value ones. If we can predict the outcome of high-value loans with great accuracy, but with worse overall accuracy than a model that predicts with high overall accuracy, but worse accuracy with regards to high-value loans, then we will likely make more money. We'll ignore these intricacies here, however, and look only at ratios of negatives and positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we also discussed above the class imbalance in the loan_status column. There are far more loans that were paid off than were not paid off. This is an issue when evaluating a model's accuracy because a model could predict 1 for every row and still appear to have a high accuracy (in our case, it would be correct more than 85% of the time).\n",
    "\n",
    "With this in mind, optimizing our model to have a high true positive rate (ratio of true positives to all actual positives (true positives + false negatives)) and a low false positive rate (ratio of false positives to all actual negatives (false positives + true negatives)) is a good way to go, as we can calculate what true positive rate and false positive rates we need to be profitable, based on the average amount we profit from a successful loan minus the average amount we lose from a failed loan.\n",
    "\n",
    "Given that our problem is one of binary classification, we'll begin by using a logistic regression model to predict whether a loan will be paid off. We'll use cross validation so that we train a logistic regression model using different train and test sets created from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_filter = (predictions == 1) & (data[\"loan_status\"] == 1)\n",
    "# Finds all true positive results\n",
    "fp_filter = (predictions == 1) & (data[\"loan_status\"] == 0)\n",
    "# Finds all true negative results\n",
    "tn_filter = (predictions == 0) & (data[\"loan_status\"] == 0)\n",
    "# Finds all false negative results\n",
    "fn_filter = (predictions == 0) & (data[\"loan_status\"] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "cols = data.columns\n",
    "train_cols = cols.drop(\"loan_status\")\n",
    "features = data[train_cols]\n",
    "target = data[\"loan_status\"]\n",
    "\n",
    "lr = LogisticRegression()\n",
    "predictions = cross_val_predict(lr, features, target, cv = 5)\n",
    "\n",
    "predictions = pd.Series(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9986645133238089 0.9976094152261861\n"
     ]
    }
   ],
   "source": [
    "tp = len(predictions[tp_filter])\n",
    "fp = len(predictions[fp_filter])\n",
    "tn = len(predictions[tn_filter])\n",
    "fn = len(predictions[fn_filter])\n",
    "\n",
    "tpr = tp / (tp + fn)\n",
    "fpr = fp / (fp + tn)\n",
    "\n",
    "print(tpr, fpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These initial numbers are terrible. We want our true positive rate (tpr) to be high, and our false positive rate (fpr) to be low. We have one, but not the other.\n",
    "\n",
    "To put these numbers more into perspective, think about what our tpr and fpr would be if our model only predicted 1's. The tpr would be 1. The model always predicts positive results, and so it will always correctly predict any positive result. Our fpr would also be 1. That is, our model would incorrectly predict every result that was actually negative.\n",
    "\n",
    "Now, we want a high tpr, but we also want a low fpr, because only with a low fpr is our model becoming more refined in how it makes its predictions. The lower the fpr, the fewer false positives we have relative to the total number of positives, and the less money we'll lose by accepting bad loans.\n",
    "\n",
    "So, we want to change our model to improve the fpr whilst still keeping a high tpr."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for why the tpr and fpr are as bad as they are, we need to look at what's going on under the hood of our model. This explanation will be brief, but I plan to cover the topic in more detail in my blog in the near future.\n",
    "\n",
    "Consider that each row in our dataset corresponds to a single data point. Logistic regression models work by determining whether that data point is more likely to belong to one of two classes, in our case, whether it represents a loan that will or will not be paid off.\n",
    "\n",
    "To establish the likelihood of a point belonging to a class, the model will plot each of the data points in Euclidean space, and define a plane which divides the data points into two groups. All of the points on one side of the plane are assigned one class (e.g. will be paid off), and the other points the other class (e.g. will not be paid off). Thus, the predicition rests entirely on the positioning of this plane.\n",
    "\n",
    "The positioning of this plane, however, is not arbitrary. The logistic regression algorithm will try out various planes based on the features passed to the algorithm (e.g. loan_amnt, term, int_rate as above). Each plane will divide the dataset into the two classes differently. How well each plane does this will be evaluated by calculating the likelihood that each datapoint in the dataset is classified correctly. The plane which does this the best is the one selected by the algorithm to use as the model, and it is from this model that predictions are made.\n",
    "\n",
    "The issue is, when the logistic regression algorithm selects a model, it selects a model that is the most accurate - that is, a model that predicts the actual value correclty most often. As it turns out, given that our target column has a class imbalance, the model that does this best is one that predicts 1's almost all of the time (that is, the plane that divides the datapoints will place almost all of the datapoints on one side of the plane).\n",
    "\n",
    "We need to fix this to improve the predictive accuracy of our model with regards to the predictions we care about - that is, the tpr and fpr. We can do this in two ways:\n",
    "- Use oversampling and undersmpaling to ensure that each time the algorithm is run, it has a balanced number of each target class\n",
    "- Tell the algorithm to penalize misclassifications of the less prevalent class (in our case, negative predictions) more than the other class.\n",
    "\n",
    "The first way to do this is difficult, since we'd need to get an equal number of 1s and 0s by one of the following:\n",
    "- use only a small portion of our positive data so we have as little of it as we do the negative data\n",
    "- copy the negative data until we have as much of it as the positive data\n",
    "- generate fake negative data until we have as much of it as the positive data\n",
    "\n",
    "The better option, therefore, is to penalize misclassifications of the negative class more strongly. We can do this by changing to class_weight parameter of the logistic regression instance to balanced. This will mean that the algorithm will be more concerned with correctly classifying rows where the loan_status value is 0. This will lower the accuracy for rows where loan_status is 1, but increase it for rows where loan_status is 0. This will likely lead to a lower tpr, but it should also lead to a lower fpr, which is very important in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_lr = LogisticRegression(class_weight = \"balanced\")\n",
    "bal_pred = cross_val_predict(balanced_lr, features, target, cv = 5)\n",
    "\n",
    "bal_pred = pd.Series(bal_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6358780048450214 0.6324016182420007\n"
     ]
    }
   ],
   "source": [
    "tp_filter = (bal_pred == 1) & (data[\"loan_status\"] == 1)\n",
    "fp_filter = (bal_pred == 1) & (data[\"loan_status\"] == 0)\n",
    "tn_filter = (bal_pred == 0) & (data[\"loan_status\"] == 0)\n",
    "fn_filter = (bal_pred == 0) & (data[\"loan_status\"] == 1)\n",
    "\n",
    "tp = len(bal_pred[tp_filter])\n",
    "fp = len(bal_pred[fp_filter])\n",
    "tn = len(bal_pred[tn_filter])\n",
    "fn = len(bal_pred[fn_filter])\n",
    "\n",
    "tpr = tp / (tp + fn)\n",
    "fpr = fp / (fp + tn)\n",
    "\n",
    "print(tpr, fpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're moving in the right direction, but we can still do better. We can reduce the false positive rate further by increasing the penalty for negative predictions. We do this by passing a dictionary to the class_weight parameter which contains the classes and their specified weightings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty = {0: 10, 1: 1}\n",
    "pen_lr = LogisticRegression(class_weight = penalty)\n",
    "pen_pred = cross_val_predict(pen_lr, features, target, cv = 5)\n",
    "pen_pred = pd.Series(pen_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20116156282998943 0.19510849577050385\n"
     ]
    }
   ],
   "source": [
    "tp_filter = (pen_pred == 1) & (data[\"loan_status\"] == 1)\n",
    "fp_filter = (pen_pred == 1) & (data[\"loan_status\"] == 0)\n",
    "tn_filter = (pen_pred == 0) & (data[\"loan_status\"] == 0)\n",
    "fn_filter = (pen_pred == 0) & (data[\"loan_status\"] == 1)\n",
    "\n",
    "tp = len(pen_pred[tp_filter])\n",
    "fp = len(pen_pred[fp_filter])\n",
    "tn = len(pen_pred[tn_filter])\n",
    "fn = len(pen_pred[fn_filter])\n",
    "\n",
    "tpr = tp / (tp + fn)\n",
    "fpr = fp / (fp + tn)\n",
    "\n",
    "print(tpr, fpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now substantially worse at predicting true positives, which means we're missing out on good loans. But, we're also much better at avoiding false positives, which will mean losing less money to bad loans.\n",
    "\n",
    "However, our ratio of tpr to fpr hasn't improved, meaning our model is still not doing any better for us in terms of our profit than the model that predicted all 1s.\n",
    "\n",
    "The above numbers mean that we're catching 1 in every 5 good loans, and we're incorrectly taking 1 in every 5 bad loans. There are around 6 times more good loans than bad loans. If we're evaluating 140 loans, 120 will be good, and 20 will be bad. We're going to take 24 of the good ones, and 4 of the bad ones, meaning 1/7 of the loans we take is bad. That's just the same as if we took every loan!\n",
    "\n",
    "If return on 6/7 loans is good enough, then that's fine, but really we'd still like to improve our model's predictive ability (and, as it happens, return on 6/7 loans is certainly not good enough).\n",
    "\n",
    "Let's use a random forest to try to improve our tpr and fpr one final time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ben\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(class_weight = \"balanced\")\n",
    "rf_pred = cross_val_predict(rf, features, target, cv = 5)\n",
    "rf_pred = pd.Series(rf_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9669234113920119 0.9639573372563442\n"
     ]
    }
   ],
   "source": [
    "tp_filter = (rf_pred == 1) & (data[\"loan_status\"] == 1)\n",
    "fp_filter = (rf_pred == 1) & (data[\"loan_status\"] == 0)\n",
    "tn_filter = (rf_pred == 0) & (data[\"loan_status\"] == 0)\n",
    "fn_filter = (rf_pred == 0) & (data[\"loan_status\"] == 1)\n",
    "\n",
    "tp = len(rf_pred[tp_filter])\n",
    "fp = len(rf_pred[fp_filter])\n",
    "tn = len(rf_pred[tn_filter])\n",
    "fn = len(rf_pred[fn_filter])\n",
    "\n",
    "tpr = tp / (tp + fn)\n",
    "fpr = fp / (fp + tn)\n",
    "\n",
    "print(tpr, fpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, using a random forest classifier seems to have actually reduced our tpr:fpr ratio, which is certainly not what we want.\n",
    "\n",
    "### 4. Conclusion\n",
    "\n",
    "None of the models we used above did a very good job predicting whether a borrower would pay back their loan. We could try to improve our models by tweaking the penalties, by trying different types of models, by using different combinations of columns, by handling columns with missing values differently, by tuning parameters, or by ensembling models of different types. For now, though, we'll leave it here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
